---
title: "KPPA CART"
author: "Fabian Bong"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Abstract

Kurtosis Projection Pursuit with Classification and Regression Trees (KPPA CART) is a framework designed for robust, unsupervised classification of integrated multi-omics datasets. It leverages kurtosis as a projection index to find latent structures in the data, followed by feature importance estimation through Classification and Regression Trees. This vignette provides an overview of the method, usage instructions, and a walk-through of a typical workflow using example data.

## Contents

Installation
Introduction to KPPACart
More Info & Citation
Example

## Installation

To install the latest development version of KPPACart from GitHub, you will need the devtools package:

```{r}
devtools::install_github("FabianBong/KPPACart")
```

Ensure that all dependencies are installed during the process.

## Introduction to KPPACart

The KPPACart package is designed with two core objectives:

1. Unsupervised clustering of samples in high-dimensional omics data using kurtosis projection pursuit.
2. Feature selection to determine which variables contribute most to the latent structure of the data.

### Method Overview

1. Subsampling of features: A random subset of features (n_features) is selected from the input data matrix X.
2. Projection Pursuit: The subset undergoes projection pursuit using kurtosis as the optimization criterion. This helps identify directions in feature space that reveal heavy-tailed (non-Gaussian) structure, which often correlates with biologically relevant subgroups.
3. Clustering: Samples are clustered in the reduced projection space using k-means (or other selected methods), with the number of expected clusters defined via exp_clusters.
4. Feature Importance: A random forest classifier is trained to predict the cluster labels using the original subset. The variable importance scores from the random forest are stored.
5. Repetition and Aggregation: Steps 1–4 are repeated n_iterations times. Importance scores are aggregated to identify consistently informative features.
6. Final Projection: A final projection is computed on the top features (based on average importance), and the samples are visualized in the reduced component space.

This approach enables discovery of intrinsic sample substructure in an unsupervised manner while highlighting the most informative variables—particularly useful in multi-omics studies.

## More Info & Citation

If you use KPPACart in your work, please cite:

Bong, F. (2025).......

For further information or to report issues, visit the GitHub repository.

## Example

The package includes an example dataset KPPACart.Data that mimics RNA-seq count data, with metadata for Age and Sex. Each of these covariates contains two groups, forming four unique sample subtypes.

### Step 1: Load the Library and Data

```{r}
library(KPPACart)

# Load example data
data("KPPACart.Data")
```

### Step 2: Compare to PCA

```{r}
pca <- prcomp(t(KPPACart.Data$X))

# Color by Age and Sex groups
group_labels <- factor(paste(KPPACart.Data$Sex, KPPACart.Data$Age, sep = "_"))

plot(pca$x[,1], pca$x[,2], col = group_labels,
     xlab = "PC1", ylab = "PC2", pch = 19,
     main = "PCA: Group Separation by Age and Sex")
legend("topright", legend = levels(group_labels), col = 1:4, pch = 19)
```

### Step 3: Run KPPACart

We now run the KPPA CART algorithm. You can adjust the parameters to suit your dataset size and computational resources. The result can be visualized to see clustering.

```{r}
set.seed(42)  # for reproducibility

res <- KPPACart::KPPACart(
  X = KPPACart.Data$X,
  n_features = 100,
  n_iterations = 1000,
  k_dim = 10,
  exp_clusters = 4,
  n_cores = 8
)

plot(res$T[,1], res$T[,2], col = group_labels,
     xlab = "Component 1", ylab = "Component 2", pch = 19,
     main = "KPPACart: Projection by Component 1 and 2")
legend("topright", legend = levels(group_labels), col = 1:4, pch = 19)
```

KPPACart typically reveals a clearer group separation compared to PCA.
